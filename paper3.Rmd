---
title: "paper3"
author: "Yi-Min Chang Chien"
date: '2022-06-02'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Package names
packages <- c("curl",
              "dplyr",
              "entropy",
              "magrittr",
              "raster",
              "readr",
              "sf",
              "stringr")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))

rm(list = c("installed_packages", "packages"))
```

```{r}
#### Data preparation
## Download the boundary of Wales
temp <- tempfile()
temp2 <- tempfile()
download.file("https://datashare.is.ed.ac.uk/bitstream/handle/10283/2410/Wales_boundary.zip", temp)
unzip(zipfile = temp, exdir = temp2)
"Wales boundary.shp" %>%
  file.path(temp2, .) %>%
  st_read(stringsAsFactors = FALSE) %>% 
  st_transform(crs = 27700) %>%
  st_combine() ->
  Wales
rm(list = c("temp", "temp2"))

## Download the visual and sensory aspect of the LANDMAP data  
VS <- read_sf("http://lle.gov.wales/catalogue/item/LandmapVisualSensory.json") %>%
  st_make_valid() %>%
  mutate(ScenicQuality = factor(VS_46, levels = c("Low", "Moderate", "High", "Outstanding")),
             Integrity = factor(VS_47, levels = c("Low", "Moderate", "High", "Outstanding")),
             Character = factor(VS_48, levels = c("Low", "Moderate", "High", "Outstanding")),
                Rarity = factor(VS_49, levels = c("Low", "Moderate", "High", "Outstanding"))) 

## Download the wildness measure and its components
#"https://github.com/yiminchangchien/scenicness/blob/2a3f9b996c11a7c480867524664401dbea9aea2b/Wildness.RData" %>% url() %>%
  
load("C:\\Users\\00631\\Downloads\\James\\Wildness.RData")

## Download Scenic-Or-Not data
sc <- read_tsv("http://scenicornot.datasciencelab.co.uk/votes.tsv",
               col_types = cols("ID" = col_number(),
                                "Lat" = col_double(),
                                "Lon" = col_double(),
                                "Average" = col_double(),
                                "Variance" = col_double(),
                                "Votes" = col_character(),
                                "Geograph URI" = col_character())) %>%
  rowwise() %>%
  mutate(Mean = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% mean(),
         Variance = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% var(),
         Entropy = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% entropy(),
         Number = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% na.omit() %>% length())%>%
  st_as_sf(coords = c("Lon","Lat"), crs = 4326) %>%
  st_transform(crs = 27700)
```

```{r}
LANDMAP <- sc %>% 
  st_join(VS, .) %>% 
  aggregate(Votes~UID, data = ., paste0, collapse = ",") %>%
  merge(VS, ., by = "UID", all = T) %>%
  as_tibble() %>% 
  st_as_sf() %>%
  rowwise() %>%
  mutate(Scenicness.median = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% median(),
         Scenicness.mean = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% mean(),
         Scenicness.entropy = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% entropy(),
         Scenicness.number = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% na.omit() %>% length(),
    Consultant = ifelse(str_detect(VS_1a, "Bronwen Thomas"), "B", "A")) %>%
  dplyr::select(UID, CLS_1, CLS_2, CLS_3, CLS_4, ScenicQuality, Consultant, Votes, Scenicness.median, Scenicness.mean, Scenicness.entropy, Scenicness.number) %>%
  ungroup()

plot(LANDMAP["ScenicQuality"], border = 'NA', key.pos = 1)  
```

Add wildness measures

```{r}
library(exactextractr)

Wildness <- raster('Z:\\R\\R projects\\LCA-GB\\James\\mce_eq256')

LANDMAP <- st_transform(LANDMAP, crs = st_crs(Wildness)) %>%
  exact_extract(Wildness, ., fun = c('mean', 'median', 'coefficient_of_variation', 'variance', 'quantile'), quantiles = c(0.25, 0.75)) %>%
  mutate(Wildness.iqr = q75 - q25) %>%
  dplyr::select(Wildness.mean = mean, Wildness.median = median, Wildness.iqr, Wildness.cv = coefficient_of_variation, Wildness.variance = variance) %>%
  bind_cols(LANDMAP, .) %>%
  st_transform(crs = 27700)


landmap <- st_cast(LANDMAP, "POLYGON")
```

```{r}
library(reticulate)

```

```{python}

import geopandas as gpd
import pandas as pd
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import make_column_transformer


# landmap = geopandas.read_file("/Users/mac/R/LCA-GB/Data/Input/LANDMAP.geojson")
landmap = r.landmap
landmap = landmap.dropna(subset=['ScenicQuality'])

print(landmap)
print(landmap.describe)
print(landmap['ScenicQuality'].value_counts())

landmap.isnull().sum()

landmap_A = landmap.query("Consultant in ['A']")
landmap_B = landmap.query("Consultant in ['B']")

print(landmap.shape)
print(landmap_A.shape)
print(landmap_B.shape)

# def split_consultant(landmap):
#   X = landmap.loc[:, ['Scenicness.mean', 'Scenicness.entropy', 'Wildness.median', 'Wildness.iqr']]
#   y = landmap.loc[:, ['ScenicQuality']]
# 
#   # Instantiate the label encoder
#   transformer = make_column_transformer(
#     (OrdinalEncoder(categories=[['Low', 'Moderate', 'High', 'Outstanding']]), ['ScenicQuality'])
#   )
# 
#   y = transformer.fit_transform(y)
#   
#   y = pd.DataFrame(y, columns = ['ScenicQuality'])
#   return X, y
# 
# def EDA_plot(y, X):
#   import pandas as pd
#   # concatenating y and X along columns
#   landmap_concat = pd.DataFrame(pd.concat([y.reset_index(drop=True), X.reset_index(drop=True)], axis=1))
#   print(landmap_concat.shape)
# 
#   import seaborn as sns
#   sns.set()
#   sns.pairplot(landmap_concat, hue='ScenicQuality', palette='vlag', height=3)

# split train/test set
X = landmap.loc[:, ['Scenicness.mean']]
y = landmap.loc[:, ['ScenicQuality']]

# from sklearn.preprocessing import OrdinalEncoder
# OE = OrdinalEncoder()
# y[['Scale']] = OE.fit_transform(y[['ScenicQuality']])

scale_mapper = {'Low': int(0), 'Moderate': int(1), 'High': int(2), 'Outstanding': int(3)} 
y[['ScenicQuality']] = y[['ScenicQuality']].replace(scale_mapper).astype('int')

# X, y = split_consultant(landmap)
print(y.shape, X.shape)
X.head()
y.head()
# EDA_plot(y, X)

from sklearn.model_selection import train_test_split
# Create train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2046, stratify=y)

```

## Hyperparameter Tuning

The optimization process consists of 4 parts which are as follows:

#### 1. Initialize domain space for range of values

The domain space is the input values over which we want to search.

Choose hyperparameter domain to search over

-   `eta` [0,1] is analogous to learning rate in GBM conceived as the step size shrinkage.

-   `n_estimators` [X/L/C]: no. of boosting iterations

-   `gamma` [0,∞] specifies the minimum loss reduction required to make a split.

-   `max_depth` [0,∞] refers to the maximum depth of a tree, same as GBM, which is used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.

-   `min_child_weight` [0,∞] is similar to min_child_leaf in GBM but not exactly which refers to min 'sum of weights' of observations while GBM has min 'number of observations.'

-   `max_delta_step` [0,∞] is usually not needed, but it might help in logistic regression when class is extremely imbalanced.

-   `subsample` [0,1] denotes the fraction of observations to be randomly samples for each tree.

-   `colsample_bytree`

-   `lamda` denotes L2 regularization term on weights (analogous to Ridge regression).

-   `alpha` denotes L1 regularization term on weights (analogous to Lasso regression), which can be used in case of very high dimensionality so that the algorithm runs faster when implemented.

-   `tree_method`

-   `scale_pos_weight`

-   `max_leaves`

```{python}
%time
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval
import numpy as np

# configuration spaces/structures using dictionary container type
space = {
  # General Parameters
           'booster': 'gbtree',
     # 'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart']),
     #     'verbosity':
     #       'nthread': 
             
  # Booster Parameters      
               'eta': hp.uniform('eta', 0.01, 1.01),
             'gamma': hp.quniform('gamma', 0.1, 5, 0.05), 
         'max_depth': hp.choice('max_depth', np.arange(1, 30, 1, dtype=int)),
  'min_child_weight': hp.choice('min_child_weight', np.arange(1, 30, 1, dtype=int)),     
         'subsample': hp.quniform('subsample', 0.3, 1.01, 0.05),
  'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
             'lamda': hp.uniform('lambda', 0.0, 1.0),
             'alpha': hp.quniform('alpha', 40, 180, 1),
  #      'tree_method': 
  # 'scale_pos_weight':
  #       'max_leaves':
          
      # 'n_estimators': hp.quniform('n_estimators', 5, 35, 1),
        # 'num_leaves': hp.quniform('num_leaves', 5, 50, 1),
     
          
  # Learning Task Parameters  
         'objective': 'multi:softmax',
       'eval_metric': 'mlogloss',
              'seed': 2046
    }

# Sample values from these configuration spaces using the routines in hyperopt.pyll.stochastic
from hyperopt.pyll.stochastic import sample
space = {'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1))}
print(sample(space))
```

#### 2. Define objective function

The objective function can be any function which returns a real value that we want to minimize. In this case, we want to minimize the validation error of a machine learning model with respect to the hyperparameters. If the real value is accuracy, then we want to maximize it. Then the function should return the negative of that metric.

```{python}
from sklearn.model_selection import cross_val_score
from hyperopt import STATUS_OK
import lightgbm as lgb

def objective_function(params):
    clf = lgb.LGBMClassifier(**params)
    score = cross_val_score(clf, X_train, y_train, cv=5).mean()
    return {'loss': -score, 'status': STATUS_OK} 
  

def score(params):
    model = XGBRegressor(**params)
    
    model.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)],
              verbose=False, early_stopping_rounds=10)
    Y_pred = model.predict(X_valid).clip(0, 20)
    score = sqrt(mean_squared_error(Y_valid, Y_pred))
    print(score)
    return {'loss': score, 'status': STATUS_OK}    
    
def optimize(trials, space):
    
    best = fmin(score, space, algo=tpe.suggest, max_evals=1000)
    return best
  
trials = Trials()
best_params = optimize(trials, space)

# Return the best parameters
space_eval(space, best_params)
  
```

From Li Ziqi

```{python}

def score(params, n_folds=5):
    
    #Cross-validation
    d_train = xgboost.DMatrix(X_coords,y)
    
    cv_results = xgboost.cv(params, d_train, nfold = n_folds, num_boost_round=500,
                        early_stopping_rounds = 10, metrics = 'rmse', seed = 0)
    
    loss = min(cv_results['test-rmse-mean'])
    
    return loss


def optimize(trials, space):
    
    best = fmin(score, space, algo=tpe.suggest, max_evals=2000,
                rstate=np.random.RandomState(333))#Add seed to fmin function
    return best
```

#### 3. Optimization algorithm

It is the method used to construct the surrogate objective function and choose the next values to evaluate.

Hyperopt has the TPE option along with random search. During optimization, the TPE algorithm constructs the probability model from the past results and decides the next set of hyperparameters to evaluate in the objective function by maximizing the expected improvement.

```{python}
# Step 3: Optimization algorithm (Tree Parzen Estimator, tpe)
from hyperopt import tpe

tpe_algorithm = tpe.suggest
```

#### 4. Results

Results are score or value pairs that the algorithm uses to build the model.

## Fit xgboost model

```{python}
import xgboost as xgb
# Specify sufficient boosting iterations to reach a minimum
num_round = 1000 #209.91

# Leave most parameters as default
param = {'objective': 'multi:softmax',  # Specify multiclass classification
         'num_class': 4,  # Number of possible output classes
         # 'tree_method': 'gpu_hist',  # Use GPU accelerated algorithm
         'eta': 0.451,
         'max_depth': 12,
         'gamma': 0,
         'subsample': 0.758,
         'colsample_bytree': 1,
         'min_child_weight': 0.2,
         'tree_method': 'hist',
         'predictor': 'cpu_predictor'
         }

# Convert input data from numpy to XGBoost format
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

import time
gpu_res = {}  # Store accuracy result
start = time.time()

# Train model
# model = xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')], evals_result=gpu_res)
# print("GPU Training Time: %s seconds" % (str(time.time() - start)))
model = xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')])
print(time.time() - start)

# Evaluate model using test data
from sklearn.metrics import accuracy_score
predictions = model.predict(dtest)
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

from sklearn.metrics import cohen_kappa_score
kappa = cohen_kappa_score(y_test, predictions)
print("Kappa: %.2f%%" % (kappa))

# save to JSON
model.save_model("/Users/mac/PycharmProjects/XGBoost/models/model_A.json")
```

## SHAP analysis of xgboost results

```{python}
explainer_shap = shap.TreeExplainer(final_model)
shap_values = explainer_shap(X_coords)

shap_interaction_values = shap.TreeExplainer(
    final_model).shap_interaction_values(X_coords)
    
X_names = ['pct_18_34','pct_white','pct_bach',
          'pct_no_car','log_pop_den','job_entropy','network_den',
          'ave_trip_dist','pct_share','x_coord','y_coord']

fig, ax = plt.subplots(figsize=(8, 6),dpi=160)

shap.summary_plot(shap_interaction_values, X_coords, max_display=15, 
                  feature_names = X_names,
                  plot_type="compact_dot")
```
