---
title: "paper3"
author: "Yi-Min Chang Chien"
date: '2022-06-02'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Package names
packages <- c("curl",
              "dplyr",
              "entropy",
              "magrittr",
              "raster",
              "readr",
              "sf",
              "stringr")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))

rm(list = c("installed_packages", "packages"))
```

## Bayesian XXX by Evan Miller

Where zα/2 is the 1−α/2 quantile of a normal distribution. - Evan Miller shows a Bayesian approach to ranking 5-star ratings: - <https://stackoverflow.com/questions/1411199/what-is-a-better-way-to-sort-by-a-5-star-rating/40958702#40958702>

```{r Bayesian Ranking}
"
Following https://www.evanmiller.org/ranking-items-with-star-ratings.html
return: the lower bound of the confidence interval
"
BayesRank <- function(votes, K, α = .05){
  N = votes %>% strsplit(",") %>% unlist() %>% length() # total_ratings
  s_k = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) %>% array(dim = c(1, 10))
  n_k = votes %>%
    strsplit(",") %>% 
    unlist() %>%
    as.numeric() %>%
    tabulate(nbins = 10) %>%
    array(dim = c(10, 1))
  # 1-α: confidence level (commonly 90%, 95% or 99%)
  # confidence level 90%: α = 0.1  -> z_α/2 = z_0.05 = 1.645
  # confidence level 95%: α = 0.05 -> z_α/2 = z_0.025= 1.96
  # confidence level 99%: α = 0.01 -> z_α/2 = z_0.005= 2.576
  # z is the standard normal random variable. z_α/2 is the α/2 quantile of its distribution which means     critical value.
  z = qnorm(1-α/2)
  S = s_k %*% (n_k + 1)/(N + K) - 
  z*sqrt((s_k^2 %*% (n_k + 1)/(N + K) - (s_k %*% (n_k + 1)/(N + K))^2)/(N + K + 1))
  S = as.numeric(S)
  return(S)
}
```

```{r}
#### Data preparation
## Download the boundary of Wales
temp <- tempfile()
temp2 <- tempfile()
download.file("https://datashare.is.ed.ac.uk/bitstream/handle/10283/2410/Wales_boundary.zip", temp)
unzip(zipfile = temp, exdir = temp2)
"Wales boundary.shp" %>%
  file.path(temp2, .) %>%
  st_read(stringsAsFactors = FALSE) %>% 
  st_transform(crs = 27700) %>%
  st_combine() ->
  Wales
rm(list = c("temp", "temp2"))

## Download the visual and sensory aspect of the LANDMAP data  
VS <- read_sf("http://lle.gov.wales/catalogue/item/LandmapVisualSensory.json") %>%
  st_make_valid() %>%
  mutate(ScenicQuality = factor(VS_46, levels = c("Low", "Moderate", "High", "Outstanding")),
         Consultant = ifelse(str_detect(VS_1a, "Bronwen Thomas"), "B", "A")) %>%
             # Integrity = factor(VS_47, levels = c("Low", "Moderate", "High", "Outstanding")),
             # Character = factor(VS_48, levels = c("Low", "Moderate", "High", "Outstanding")),
             #    Rarity = factor(VS_49, levels = c("Low", "Moderate", "High", "Outstanding"))) %>%
  dplyr::select(UID, CLS_1, CLS_2, CLS_3, CLS_4, Consultant, ScenicQuality)

## Download Scenic-Or-Not data
sc <- read_tsv("http://scenicornot.datasciencelab.co.uk/votes.tsv",
               col_types = cols("ID" = col_number(),
                                "Lat" = col_double(),
                                "Lon" = col_double(),
                                "Average" = col_double(),
                                "Variance" = col_double(),
                                "Votes" = col_character(),
                                "Geograph URI" = col_character())) %>%
  rowwise() %>%
  mutate(Mean = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% mean(),
         Median = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% median(),
         Variance = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% var(),
         Entropy = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% entropy(),
         BayeScore = Votes %>% BayesRank(K = 10, α = .05),
         Number = Votes %>% strsplit(",") %>% unlist() %>% length())%>%
  st_as_sf(coords = c("Lon","Lat"), crs = 4326) %>%
  st_transform(crs = 27700)
```

```{r}
LANDMAP <- sc %>% 
  st_join(VS, .) %>%
  aggregate(Votes~UID, data = ., paste0, collapse = ",") %>%
  merge(VS, ., by = "UID", all = T) %>%
  as_tibble() %>% 
  rowwise() %>%
  mutate(Scenicness.number_of_all = strsplit(Votes, ",") %>% unlist() %>% na.omit() %>% length(),
         Scenicness.mean_of_all = strsplit(Votes, ",") %>% unlist() %>% as.numeric() %>% mean(),
         Scenicness.median_of_all = strsplit(Votes, ",") %>% unlist() %>% as.numeric() %>% median(),
         Scenicness.entropy_of_all = strsplit(Votes, ",") %>% unlist() %>% as.numeric() %>% entropy()) %>%
  ungroup() %>%
  st_as_sf()

LANDMAP <- sc %>%
  st_join(LANDMAP, .) %>%
  aggregate(Median~UID, data = ., paste0, collapse = ",") %>%
  merge(LANDMAP, ., by = "UID", all = T) %>%
  as_tibble() %>%
  rowwise() %>%
  mutate(Scenicness.mean_of_median = strsplit(Median, ",") %>% unlist() %>% as.numeric() %>% mean(),
         Scenicness.median_of_median = strsplit(Median, ",") %>% unlist() %>% as.numeric() %>% median()) %>%
  dplyr::select(-Median) %>%
  ungroup() %>%
  st_as_sf()

LANDMAP <- sc %>%
  st_join(LANDMAP, .) %>%
  aggregate(Mean~UID, data = ., paste0, collapse = ",") %>%
  merge(LANDMAP, ., by = "UID", all = T) %>%
  as_tibble() %>%
  rowwise() %>%
  mutate(Scenicness.mean_of_mean = strsplit(Mean, ",") %>% unlist() %>% as.numeric() %>% mean(),
         Scenicness.median_of_mean = strsplit(Mean, ",") %>% unlist() %>% as.numeric() %>% median()) %>%
  dplyr::select(-Mean) %>%
  ungroup() %>%
  st_as_sf()

LANDMAP <- sc %>%
  st_join(LANDMAP, .) %>%
  aggregate(cbind(BayeScore, Number, VR = BayeScore*Number)~UID, data = ., paste0, collapse = ",") %>%
  merge(LANDMAP, ., by = "UID", all = T) %>%
  as_tibble() %>%
  rowwise() %>%
  mutate(Scenicness.mean_of_bayescore = strsplit(BayeScore, ",") %>% unlist() %>% as.numeric() %>% mean(),
         Scenicness.median_of_bayescore = strsplit(BayeScore, ",") %>% unlist() %>% as.numeric() %>% median(),
         Score_multiplied_by_Number = strsplit(VR, ",") %>% unlist() %>% as.numeric() %>% sum(na.rm = TRUE),
         Scenicness.weighted_bayescore =  Score_multiplied_by_Number/(Scenicness.number_of_all + 3) + 
                                          3*Scenicness.mean_of_bayescore/(Scenicness.number_of_all + 3)) %>%
  dplyr::select(-c(BayeScore, Number, VR, Score_multiplied_by_Number)) %>%
  ungroup() %>%
  st_as_sf()

# plot(LANDMAP["ScenicQuality"], border = 'NA', col = RColorBrewer::brewer.pal(4, "YlOrBr"), key.pos = 1)  
```

Add wildness measures

```{r}
library(exactextractr)

Wildness <- raster('Z:\\R\\R projects\\LCA-GB\\James\\mce_eq256')

LANDMAP <- st_transform(LANDMAP, crs = st_crs(Wildness)) %>%
  exact_extract(Wildness, ., 
                fun = c('mean', 'median', 'coefficient_of_variation', 'variance', 'quantile'), 
                quantiles = c(0.25, 0.75)) %>%
  mutate(Wildness.mean = mean,
         Wildness.median = median,
         Wildness.iqr = q75 - q25,
         Wildness.cv = coefficient_of_variation,
         Wildness.variance = variance) %>%
  dplyr::select(-c(mean, median, q25, q75, coefficient_of_variation, variance)) %>%
  bind_cols(LANDMAP, .) %>%
  st_transform(crs = 27700)

landmap <- st_cast(LANDMAP, "POLYGON") %>%
  dplyr::mutate(UID = row_number(),
                x_coord = st_centroid(landmap$geometry) %>% st_coordinates() %>% .[, 1],
                y_coord = st_centroid(landmap$geometry) %>% st_coordinates() %>% .[, 2])
summary(landmap)
```

Separate the consultant and then split train-test sets
```{r}
landmap_A = landmap %>% filter(Consultant == "A")
landmap_B = landmap %>% filter(Consultant == "B")

library(rsample)

# Set the random number stream using `set.seed()` so that the results can be 
# reproduced later. 
set.seed(2046)

# Save the split information for an 80/20 split of the data
split <- landmap_B %>% 
  filter(!is.na(ScenicQuality)) %>% 
  initial_split(prop = 0.80, strata = ScenicQuality)

train_set <- training(split) %>% st_drop_geometry() 
 test_set <- testing(split) %>% st_drop_geometry() 
dim(train_set)
dim(test_set) 

X_train <- train_set %>% 
  dplyr::select(Scenicness = Scenicness.weighted_bayescore,
                Wildness = Wildness.median, 
                x_coord, y_coord)
 X_test <-  test_set %>% 
   dplyr::select(Scenicness = Scenicness.weighted_bayescore,
                 Wildness = Wildness.median, 
                 x_coord, y_coord)

y_train <- train_set %>% dplyr::select(ScenicQuality)
 y_test <-  test_set %>% dplyr::select(ScenicQuality)  
```

```{r}
Sys.setenv("RETICULATE_PYTHON" = ".\\.venv\\Scripts\\python.exe")
# Sys.getenv()
Sys.which("Python")

# py_discover_config()

library(reticulate)
# -- creates an interactive Python console, REPL (Read, Eval, Print Loop), within R.
repl_python() 
```


```{python}
# -- load in the train/test sets
X_train = r.X_train
y_train = r.y_train
 X_test = r.X_test
 y_test = r.y_test
 
scale_mapper = {'Low': int(0),
                'Moderate': int(1),
                'High': int(2),
                'Outstanding': int(3)}
y_train[['ScenicQuality']] = y_train[['ScenicQuality']].replace(scale_mapper).astype('int')
 y_test[['ScenicQuality']] = y_test[['ScenicQuality']].replace(scale_mapper).astype('int')
```
## Hyperparameter Tuning

The optimization process consists of 4 parts which are as follows:

#### 1. Define a search space

The domain space is the input values over which we want to search. Choose hyperparameter domain to search over

-   `eta` [0,1] is analogous to learning rate in GBM conceived as the step size shrinkage.
-   `n_estimators` [X/L/C]: no. of boosting iterations
-   `gamma` [0,∞] specifies the minimum loss reduction required to make a split.
-   `max_depth` [0,∞] refers to the maximum depth of a tree, same as GBM, which is used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.
-   `min_child_weight` [0,∞] is similar to min_child_leaf in GBM but not exactly which refers to min 'sum of weights' of observations while GBM has min 'number of observations.'
-   `max_delta_step` [0,∞] is usually not needed, but it might help in logistic regression when class is extremely imbalanced.
-   `subsample` [0,1] denotes the fraction of observations to be randomly samples for each tree.
-   `colsample_bytree`
-   `lamda` denotes L2 regularization term on weights (analogous to Ridge regression).
-   `alpha` denotes L1 regularization term on weights (analogous to Lasso regression), which can be used in case of very high dimensionality so that the algorithm runs faster when implemented.
-   `tree_method`
-   `scale_pos_weight`
-   `max_leaves`

```{python}
# -- hyperopt for optimising a XGBoost model
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval
from sklearn.metrics import cohen_kappa_score
import numpy as np
import xgboost as xgb

# -- define a search space
param_space = {
  # General Parameters
           'booster': 'gbtree',
     # 'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart']),
     #     'verbosity':
     #       'nthread': 
             
  # Booster Parameters
   'num_boost_round': hp.quniform('num_boost_round', 100, 1500, 0.001), 
               'eta': hp.uniform('eta', 0.01, 1.01),
             'gamma': 0, 
         'max_depth': hp.choice('max_depth', np.arange(1, 10, 1, dtype=int)),
  'min_child_weight': hp.quniform('min_child_weight', 0, 1, 0.1),     
         'subsample': hp.quniform('subsample', 0.3, 1, 0.001),
  'colsample_bytree': 1, #hp.uniform('colsample_bytree', 0.5, 1.01),
 #       'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),
 #        'reg_alpha': hp.quniform('reg_alpha', 40, 180, 1),
 #      'tree_method': 'hist','gpu_hist',  # Use GPU accelerated algorithm
 # 'scale_pos_weight':
 #       'max_leaves':
 #     'n_estimators': hp.quniform('n_estimators', 5, 35, 1),
 #       'num_leaves': hp.quniform('num_leaves', 5, 50, 1),
 #        'predictor': 'cpu_predictor',
          
  # Learning Task Parameters
         'num_class': 4,                   # Number of possible output classes
         'objective': 'multi:softmax',     # Specify multi-class classification
       'eval_metric': 'merror',
              'seed': 2022
    }

# Sample values from these configuration spaces using the routines in hyperopt.pyll.stochastic
from hyperopt.pyll.stochastic import sample
space = {'eta': hp.uniform('eta', 0.01, 1.01)}
print(sample(space))
```

#### 2. Define an objective/loss/cost function

The objective function can be any function which returns a real value that we want to minimize. In this case, we want to minimize the validation error of a machine learning model with respect to the hyperparameters. If the real value is accuracy, then we want to maximize it. Then the function should return the negative of that metric.

```{python}
# -- define an objective function
def objective(params, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test):
    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train,
              eval_set = [(X_train, y_train), (X_test, y_test)])
    y_pred = model.predict(X_test)
    score = np.mean(cohen_kappa_score(y_test, y_pred))
    # -- return more than just the objective value
    return {'loss': score, 'status': STATUS_OK}
```

##### Implementing/Choosing the evaluation metric:

#### 3. Minimize the objective over the space (optimisation algorithm)

It is the method used to construct the surrogate objective function and choose the next values to evaluate. The optimization algorithm is based on the Sequential Model-Based Global Optimization (SMBO) methodology with the variants given by the Gaussian Processes (GP), Tree of Parzen Estimators (TPE) and Adaptive Tree of Parzen Estimators (ATPE) algorithms.

Hyperopt has the TPE option along with random search. During optimization, the TPE algorithm constructs the probability model from the past results and decides the next set of hyperparameters to evaluate in the objective function by maximizing the expected improvement.

##### Parameter Tuning: Optimizing the Cross Validated Score

We now need to put these together and call Hyperopt's optimization function, fmin, to search for the optimal parameter combination. I will define a new function that implements this optimization and returns the parameters that acheive the highest cross validated score.

The fmin function minimzes the objective function over the paramter space defined by param_space (described below). It takes a Trials object to store the results of all iterations, which can be used later to gain insights about the search process and results. For more about this (and Tree-structured Parzen Estimator, TPE, which Hyperopt uses internally for optimization) see Hyperopt documentation.

```{python}
# -- optimize it using algorithm of choice
def optimize(trials, space):
    best = fmin(fn = objective,
                space = param_space, 
                algo = tpe.suggest,    # the optimisation algorithm
                max_evals = 30,        # the number of iteration
                trials = trials)       # the trials database for fmin
    return best

# -- create a Trials database to store experiment results
trials = Trials()
best_params = optimize(trials, param_space)

# Return the best parameters
space_eval(param_space, best_params)
```

#### 4. Results

Results are score or value pairs that the algorithm uses to build the model.

## Fit/Train the xgboost model

```{python}
import xgboost as xgb
## Specify sufficient boosting iterations to reach a minimum
## num_round = 1000

# -- convert input data from numpy to XGBoost format
dtrain = xgb.DMatrix(X_train, label = y_train)
 dtest = xgb.DMatrix(X_test, label = y_test)

# -- try previous combinations of hyperparameter set
# Consultant A
# param_space = {
#            'booster': 'gbtree',
#    'num_boost_round': 209.910,
#                'eta': 0.451,
#              'gamma': 0,
#          'max_depth': 12,
#   'min_child_weight': 0.2,
#          'subsample': 0.758,
#   'colsample_bytree': 1,
#          'num_class': 4,
#          'objective': 'multi:softmax',
#        'eval_metric': 'merror',
#               'seed': 2046
#     }

# Consultant B
params = {'booster': 'gbtree',
        'num_round': 1434, #1433.996,
              'eta': 0.892,
            'gamma': 0,
        'max_depth': 4,
 'min_child_weight': 0.6,
        'subsample': 0.500,
 'colsample_bytree': 1,
        'num_class': 4,
        'objective': 'multi:softmax',
      'eval_metric': 'merror',
             'seed': 1995}

# -- train model
import time
start = time.time()

# model = xgb.train(space_eval(param_space, best_params),
#                   dtrain,
#                   num_boost_round = 500,
#                   evals = [(dtrain, 'train'), (dtest, 'test')],
#                   early_stopping_rounds = 250)

model = xgb.train(params,
                  dtrain,
                  num_boost_round = 500,
                  evals = [(dtrain, 'train'), (dtest, 'test')],
                  verbose_eval = 100,
                  early_stopping_rounds = 251)

# model = xgb.cv(space_eval(param_space, best_params), dtrain, num_boost_round = 2000, nfold = 5, stratified = True)
# model.head()
                  
print("XGBoost model training time: %s seconds" % (str(time.time() - start)))

y_pred = model.predict(dtest)
y_pred = y_pred.astype(int)
y_true = y_test.values.flatten()

# -- evaluate model using test data
# -- overall accuracy
from sklearn.metrics import accuracy_score
y_actual = y_test.values.flatten()
accuracy = accuracy_score(y_true, y_pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

# -- Cohen's kappa 
from sklearn.metrics import cohen_kappa_score
kappa = cohen_kappa_score(y_true, y_pred)
print("Kappa: %.2f" % (kappa))

# -- save to JSON
model.save_model("Z:\\R\\R projects\\LCA-GB\\models\\model.json")
```

## SHAP analysis of xgboost results

```{python, engine.path = '.venv/Scripts/python'}
import shap
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.pylab as pl
import xgboost as xgb
from sklearn.preprocessing import StandardScaler

# print the JS visualization code to the notebook
shap.initjs()

# -- load the saved model
model = xgb.XGBClassifier()
model.load_model("Z:\\R\\R projects\\LCA-GB\\models\\model.json")
```

## Confusion matrix and its visualisation
```{python}
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test.values, y_pred)
# -- view the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix = cm, 
                              display_labels = ['Low', 'Moderate', 'High', 'Outstanding'])
disp.plot(cmap = plt.cm.Blues)
plt.show(); plt.close()
```

```{python plot}
X =  r.landmap.loc[:, ['Scenicness.weighted_bayescore', 'Wildness.median', 'x_coord', 'y_coord']]
# -- play shap
explainer = shap.TreeExplainer(model)

# The approximate argument set to True to fix some of the shap_values with NaN value
# shape of shap_values is : (4, 1321, 4) means 4 classes, 1321 rows(observations), each 4 columns(features).
shap_values = explainer.shap_values(X, approximate = True)

# shap.plots.beeswarm(shap_values, show = True, color_bar = False)
# plt.colorbar()
# plt.show()

# In the case of matplotlib version 3.5 above, the adjustment of colorbar legend was referred to https://stackoverflow.com/questions/70461753/shap-the-color-bar-is-not-displayed-in-the-summary-plot/
# summary plot for "CLASS 0" (Low scenic quality) 

# landmap = r.landmap
X_vars = ['Scenicness','Wildness', 'x_coord', 'y_coord']

import matplotlib.pyplot as plt

fig, ax = plt.subplots(2,2,figsize=(15,15))

for j in range(len(shap_values)):
    ax = ax.ravel()
    # X.plot(ax=ax[j],column = shap_values.values[:,j],legend=True,
    #              vmin=-0.8,vmax=0.8,cmap=shap.plots.colors.red_white_blue)
    fig[j] = shap.summary_plot(shap_values[1],
                              X.values,
                              feature_names = X_vars,
                              show = False),
                              # plot_type = 'compact_dot', # making the dots disappear
                              plot_size = (50, 6))
             plt.gcf().axes[-1].set_aspect('auto')
             plt.tight_layout()
             plt.gcf().axes[-1].set_box_aspect(40)
    ax[j].set_title("SHAP for\n" + X_vars[j],fontsize=10)
    
fig.delaxes(ax[-1])


shap.summary_plot(shap_values[1],
                  X.values,
                  feature_names = ['Scenicness', 'Wildness', 'x_coord', 'y_coord'], show = False),
                  # plot_type = 'compact_dot', # making the dots disappear
                  plot_size = (50, 6))
plt.gcf().axes[-1].set_aspect('auto')
plt.tight_layout()
plt.gcf().axes[-1].set_box_aspect(40) # smaller "box_aspect" value to make colorbar thicker
plt.show(); plt.close()
# plt.savefig('fig_tes1.png', bbox_inches='tight',dpi=300); plt.close()
```
## Spatial patterns of SHAP values
```{r}
shap_values = py$shap_values
shap_Low = data.frame(shap_values[[1]]) %>% transmute(SHAP_Scenicness_Low = X1, 
                                                      SHAP_Wildness_Low = X2,
                                                      SHAP_Location_Low = X3 + X4) 
shap_Moderate = data.frame(shap_values[[2]]) %>% transmute(SHAP_Scenicness_Moderate = X1,
                                                           SHAP_Wildness_Moderate = X2,
                                                           SHAP_Location_Moderate= X3 + X4)
shap_High = data.frame(shap_values[[3]]) %>% transmute(SHAP_Scenicness_High = X1,
                                                       SHAP_Wildness_High = X2,
                                                       SHAP_Location_High = X3 + X4)
shap_Outstanding = data.frame(shap_values[[4]]) %>% transmute(SHAP_Scenicness_Outstanding = X1,
                                                              SHAP_Wildness_Outstanding = X2,
                                                              SHAP_Location_Outstanding = X3 + X4)
   
landmap_shap = cbind(landmap, shap_Low, shap_Moderate, shap_High, shap_Outstanding)
rm(list = c('shap_Low', 'shap_Moderate', 'shap_High', 'shap_Outstanding'))

```
## Visualising the spatial distributions of SHAP values for each covariate with each class
```{r}
library(ggplot2)

theme_map <- function(...) {
  theme_minimal() +
    theme(
      text = element_text(color = "#22211d"), # family = "Garamond", "Ubuntu Regular"
      axis.line = element_blank(),
      axis.text.x = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks = element_blank(),
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      # panel.grid.minor = element_line(color = "#ebebe5", size = 0.2),
      panel.grid.major = element_blank(), #element_line(color = "#ebebe5", size = 0.2),
      panel.grid.minor = element_blank(),
      plot.background = element_blank(), #element_rect(fill = "#f5f5f2", color = NA), 
      panel.background = element_blank(), #element_rect(fill = "#f5f5f2", color = NA), 
      legend.background = element_blank(), #element_rect(fill = "#f5f5f2", color = NA),
      panel.border = element_blank(),
      ...
    )
}

f <- list()

for (i in 1:12) {
  f[[i]] <- ggplot() +
    geom_sf(data = landmap_shap, aes(fill = !!sym(names(landmap_shap)[i+25])), colour = NA) +
    theme_map() +
    labs(x = NULL, 
         y = NULL, 
         title = names(landmap_shap)[i+25]) + # stringr::str_c("SHAP for ", collapse = " ")
    theme(legend.position = "bottom",
          title = element_text(size = 15)) +
    scale_fill_gradient2(low = ("#CB181D"),
                         mid = "white",
                         name = NULL,
                         high = ("#2171B5"),
                         midpoint = 0,
                         space = "Lab",
                         na.value = "white",
                         guide = guide_colourbar(title.position = "top",
                                                 # title.hjust = 0.5,
                                                 title.vjust = 0.8,
                                                 barwidth = 15))
}

## -- multiplot function
multiplot <- function(plot.list, file, cols=3, layout=NULL) {
	library(grid)
	numPlots = length(plot.list)
	if (is.null(layout)) {
		layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
						ncol = cols, nrow = ceiling(numPlots/cols))
	}
	if (numPlots==1) {
		print(plots[[1]])
	} else {
	  grid.newpage()
	  pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
	  for (i in 1:numPlots) {
	  	matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
	  	print(plot.list[[i]], vp = viewport(layout.pos.row = matchidx$row,
	  									layout.pos.col = matchidx$col))
	  }
	}
}

## map together
setwd("Z:\\R\\R projects\\LCA-GB\\figures")
png(filename = "SHAP_values.png", w = 10*2, h = 6*2, units = "in", res = 300)
par(mar = c(0,0,0,0))
multiplot(list(f[1], f[2], f[3], f[4], 
               f[5], f[6], f[7], f[8], 
               f[9], f[10], f[11], f[12]), cols = 4)
dev.off()
```
# Change of support (to 1 km grid)
```{r}
# -- download OSGB 1 km grid from github and covert its coordinate system to 27700
grid_1km <- "https://raw.githubusercontent.com/charlesroper/OSGB_Grids/master/GeoJSON/OSGB_Grid_1km.geojson" |>
  geojsonio::geojson_read(what = "sp") |>
  as("sf") |>
  st_transform(crs = 27700) 

st_crs(grid_1km) == st_crs(sc)

# -- spatial join OSGB 1 km grid with Scenic-Or-Not data for the sake of understanding the distribution of Scenic-Or-Not ratings

# grid_1km <- st_join(grid_1km, sc) |>
#   st_drop_geometry() |>
#   group_by(PLAN_NO) |>
#   summarise(ID = paste(ID, collapse = ','),
#             `Geograph URI` = paste(`Geograph URI`, collapse = ','),
#             Votes = paste(Votes, collapse = ','),
#             Number = paste(Number, collapse = ','),
#             Mean = paste(Mean, collapse = ','),
#             Median = paste(Median, collapse = ','),
#             BayeScore = paste(BayeScore, collapse = ','),
#             Entopy = paste(Entropy, collapse = ',')) |>
#   rowwise() |>
#   mutate(Photos = ifelse(ID == 'NA', 0, strsplit(ID, ',') %>% unlist() %>% length())) |>
#   ungroup() %>%
#   left_join(grid_1km, ., by = c('PLAN_NO' = 'PLAN_NO'))

grid_1km <- st_join(grid_1km, sc) %>%
  aggregate(cbind(BayeScore, Number, VR = BayeScore*Number)~PLAN_NO, data = ., paste0, collapse = ',') %>%
  merge(grid_1km, ., by = 'PLAN_NO', all = T) |>
  st_drop_geometry() |>
  rowwise() |>    
  mutate(Scenicness.number_of_all = strsplit(Number, ',') %>% unlist() %>% as.numeric() %>% sum(na.rm = TRUE),
         Scenicness.mean_of_bayescore = strsplit(BayeScore, ',') %>% unlist() %>% as.numeric() %>% mean(),
         Scenicness.median_of_bayescore = strsplit(BayeScore, ',') %>% unlist() %>% as.numeric() %>% median(),
         Score_multiplied_by_Number = strsplit(VR, ',') %>% unlist() %>% as.numeric() %>% sum(na.rm = TRUE),
         Scenicness.weighted_bayescore =  Score_multiplied_by_Number/(Scenicness.number_of_all + 3) + 
                                          3*Scenicness.mean_of_bayescore/(Scenicness.number_of_all + 3)) |>
  dplyr::select(-c(BayeScore, Number, VR, Score_multiplied_by_Number, Scenicness.number_of_all)) |>
  ungroup() %>%
  left_join(grid_1km, ., by = c('PLAN_NO' = 'PLAN_NO'))

# -- extract raster values of wildness quality based on 1 km OSGB grid 
library(exactextractr)

Wildness <- raster('Z:\\R\\R projects\\LCA-GB\\James\\mce_eq256')

st_crs(Wildness) == st_crs(grid_1km)
  
grid_1km <- st_transform(grid_1km, crs = st_crs(Wildness)) %>%
  exact_extract(Wildness, ., 
                fun = c('mean', 'median', 'coefficient_of_variation', 'variance', 'quantile'), 
                quantiles = c(0.25, 0.75)) |>
  mutate(Wildness.mean = mean,
         Wildness.median = median,
         Wildness.iqr = q75 - q25,
         Wildness.cv = coefficient_of_variation,
         Wildness.variance = variance) |>
  dplyr::select(-c(mean, median, q25, q75, coefficient_of_variation, variance)) %>%
  bind_cols(grid_1km, .) |>
  st_transform(crs = 27700)

# -- calculate the centroid coordinate for each grid cell
grid_1km <- mutate(grid_1km, 
                   x_coord = st_centroid(grid_1km$geometry) %>% st_coordinates() %>% .[, 1],
                   y_coord = st_centroid(grid_1km$geometry) %>% st_coordinates() %>% .[, 2])

# -- download the administrative region for Great Britain from GADM for subseting the 1 km grid
GB <- raster::getData('GADM', country = 'GBR', level = 1) |>
  as('sf') |>
  filter(!NAME_1=='Northern Ireland') |>
  st_union() |>
  st_transform(crs = 27700)
plot(GB)

grid_1km <- grid_1km[GB, ]
```
# prepare environment for running python
```{r}
Sys.setenv("RETICULATE_PYTHON" = ".\\.venv\\Scripts\\python.exe")
# Sys.getenv()
Sys.which("Python")

# py_discover_config()

library(reticulate)
# -- creates an interactive Python console, REPL (Read, Eval, Print Loop), within R.
repl_python() 
```

# SHAP value
```{python plot}
import shap

X =  r.grid_1km.loc[:, ['Scenicness.weighted_bayescore', 'Wildness.median', 'x_coord', 'y_coord']]
# -- play shap
explainer = shap.TreeExplainer(model)

# The approximate argument set to True to fix some of the shap_values with NaN value
# shape of shap_values is : (4, 1321, 4) means 4 classes, 1321 rows(observations), each 4 columns(features).
shap_values = explainer.shap_values(X, approximate = True)

# shap.plots.beeswarm(shap_values, show = True, color_bar = False)
# plt.colorbar()
# plt.show()

# In the case of matplotlib version 3.5 above, the adjustment of colorbar legend was referred to https://stackoverflow.com/questions/70461753/shap-the-color-bar-is-not-displayed-in-the-summary-plot/
# summary plot for "CLASS 0" (Low scenic quality) 

# landmap = r.landmap

# X_vars = ['Scenicness','Wildness', 'x_coord', 'y_coord']
# 
# import matplotlib.pyplot as plt
# 
# fig, ax = plt.subplots(2,2,figsize=(15,15))
# 
# for j in range(len(shap_values)):
#     ax = ax.ravel()
#     # X.plot(ax=ax[j],column = shap_values.values[:,j],legend=True,
#     #              vmin=-0.8,vmax=0.8,cmap=shap.plots.colors.red_white_blue)
#     fig[j] = shap.summary_plot(shap_values[1],
#                               X.values,
#                               feature_names = X_vars,
#                               show = False),
#                               # plot_type = 'compact_dot', # making the dots disappear
#                               plot_size = (50, 6))
#              plt.gcf().axes[-1].set_aspect('auto')
#              plt.tight_layout()
#              plt.gcf().axes[-1].set_box_aspect(40)
#     ax[j].set_title("SHAP for\n" + X_vars[j],fontsize=10)
#     
# fig.delaxes(ax[-1])
# 
# 
# shap.summary_plot(shap_values[1],
#                   X.values,
#                   feature_names = ['Scenicness', 'Wildness', 'x_coord', 'y_coord'], show = False),
#                   # plot_type = 'compact_dot', # making the dots disappear
#                   plot_size = (50, 6))
# plt.gcf().axes[-1].set_aspect('auto')
# plt.tight_layout()
# plt.gcf().axes[-1].set_box_aspect(40) # smaller "box_aspect" value to make colorbar thicker
# plt.show(); plt.close()
# plt.savefig('fig_tes1.png', bbox_inches='tight',dpi=300); plt.close()
```
## Spatial patterns of SHAP values
```{r}
shap_values = py$shap_values
shap_Low = data.frame(shap_values[[1]]) %>% transmute(SHAP_Scenicness_Low = X1, 
                                                      SHAP_Wildness_Low = X2,
                                                      SHAP_Location_Low = X3 + X4) 
shap_Moderate = data.frame(shap_values[[2]]) %>% transmute(SHAP_Scenicness_Moderate = X1,
                                                           SHAP_Wildness_Moderate = X2,
                                                           SHAP_Location_Moderate= X3 + X4)
shap_High = data.frame(shap_values[[3]]) %>% transmute(SHAP_Scenicness_High = X1,
                                                       SHAP_Wildness_High = X2,
                                                       SHAP_Location_High = X3 + X4)
shap_Outstanding = data.frame(shap_values[[4]]) %>% transmute(SHAP_Scenicness_Outstanding = X1,
                                                              SHAP_Wildness_Outstanding = X2,
                                                              SHAP_Location_Outstanding = X3 + X4)
   
grid_1km_shap = cbind(grid_1km, shap_Low, shap_Moderate, shap_High, shap_Outstanding)
rm(list = c('shap_Low', 'shap_Moderate', 'shap_High', 'shap_Outstanding'))
```
## plotting the spatial distribution of SHAP values
```{r}
library(ggplot2)

f2 <- list()

for (i in 1:12) {
  f2[[i]] <- ggplot() +
    geom_sf(data = grid_1km_shap, aes(fill = !!sym(names(grid_1km_shap)[i+11])), colour = NA) +
    theme_map() +
    labs(x = NULL, 
         y = NULL, 
         title = names(grid_1km_shap)[i+11]) + # stringr::str_c("SHAP for ", collapse = " ")
    theme(legend.position = "bottom",
          title = element_text(size = 15)) +
    scale_fill_gradient2(low = ("#CB181D"),
                         mid = "white",
                         name = NULL,
                         high = ("#2171B5"),
                         midpoint = 0,
                         space = "Lab",
                         na.value = "white",
                         guide = guide_colourbar(title.position = "top",
                                                 # title.hjust = 0.5,
                                                 title.vjust = 0.8,
                                                 barwidth = 15))
}


setwd("Z:\\R\\R projects\\LCA-GB\\figures")
png(filename = "SHAP_values_4.png", w = 10*2, h = 6*2, units = "in", res = 300)
par(mar = c(0,0,0,0))
multiplot(list(f2[1], f2[2], f2[3], f2[4], 
               f2[5], f2[6], f2[7], f2[8], 
               f2[9], f2[10], f2[11], f2[12]), cols = 4)
dev.off()
```

```{python}
# -- view the feature importance
importances = model.feature_importances_
indices = np.argsort(importances)
features = X.columns

fig, ax = plt.subplots()

plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='g', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

fig.set_size_inches(18.5, 10.5)
fig.savefig("Z:\\R\\R projects\\LCA-GB\\models\\pyplot.png")
plt.clf()
```

```{python}
# get just the explanations for the positive class
# shap_values = shap_values[...,1]

# shap.plots.bar(shap_values[...,1])
```

```{python}
shap.initjs()
# shap_values[1] what does the index mean?
shap.summary_plot(shap_values[1], X_train.values, feature_names=X_train.columns)
# shap.summary_plot(shap_values, X_test, max_display = X_test.shape[1]) 


# -- Dependence Contribution Plots
# make plot.
shap.dependence_plot('Scenicness.median_of_bayescore', shap_values[3], X_train, interaction_index = 'Wildness.median')


# explainer = shap.TreeExplainer(model2)
# shap_values = explainer.shap_values(X_sampled)
# shap.summary_plot (shap_values, X_sampled, max_display=X_sampled.shape[1]) 
# i=2
# shap.force_plot(explainer.expected_value[0], shap_values[0][i], X_test.values[i], feature_names = X_test.columns)
# 
# 
# fig.set_size_inches(18.5, 20.5)
# fig.savefig("Z:\\R\\R projects\\LCA-GB\\models\\shap.png")
# 
# # -- first trial
# shap.initjs()
# shap_values = shap.TreeExplainer(model).shap_values(X_test)
# shap.summary_plot(shap_values, X_test)
# 
# # -- second trial
# scaler = StandardScaler()
# scaled_data = scaler.fit_transform(X_test.iloc[:,:-1])
# shap_values = explainer.shap_values(scaled_data.iloc[0,:-1].values.reshape(1,-1))
# shap.force_plot(explainer.expected_value[0], shap_values[0], link='logit')  # repeat changing 0 for i in range(0, 5)
# 
# # -- third trial
# shap.initjs()
# class_names = ['Low', 'Moderate', ' High', 'Outstanding']
# shap.summary_plot(shap_values, X_test, plot_type="bar", class_names= class_names, feature_names = ['Scenicness', 'Wildness'])
# 
# # -- plot the global importance of each feature
# shap.plots.bar(explainer)
# 
# # visualize the first prediction's explanation
# shap.force_plot(explainer.expected_value[0], shap_values[0])
# 
# # fig, ax = plt.subplots(figsize=(20, 20),dpi=200)
# # shap.summary_plot(shap_values)
# # # or 
# # shap.plots.beeswarm(shap_values)
# shap.summary_plot(shap_values[2], X_test)
# 
# # fig, ax = plt.subplots(figsize=(20, 20),dpi=200)
# shap.summary_plot(shap_values, X_test, plot_type="bar")
# 
# shap_interaction_values = shap.TreeExplainer(model).shap_interaction_values(X_train)
#     
# X_names = ['Scenicness.median','x_coord','y_coord']
# 
# shap.summary_plot(shap_interaction_values, X_train, max_display=15, 
#                   feature_names = X_names,
#                   plot_type = 'dot')
#                   # plot_type="compact_dot")
#                   
# import shap  # package used to calculate Shap values
# 
# # Create object that can calculate shap values
# explainer = shap.TreeExplainer(my_model)
# 
# # calculate shap values. This is what we will plot.
# # Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
# shap_values = explainer.shap_values(val_X)
# 
# # Make plot. Index of [1] is explained in text below.
# shap.summary_plot(shap_values[1], val_X)                  
```
