---
title: "paper3"
author: "Yi-Min Chang Chien"
date: '2022-06-02'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Package names
packages <- c("curl",
              "dplyr",
              "entropy",
              "magrittr",
              "raster",
              "readr",
              "sf",
              "stringr")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))

rm(list = c("installed_packages", "packages"))
```

```{r}
#### Data preparation
## Download the boundary of Wales
temp <- tempfile()
temp2 <- tempfile()
download.file("https://datashare.is.ed.ac.uk/bitstream/handle/10283/2410/Wales_boundary.zip", temp)
unzip(zipfile = temp, exdir = temp2)
"Wales boundary.shp" %>%
  file.path(temp2, .) %>%
  st_read(stringsAsFactors = FALSE) %>% 
  st_transform(crs = 27700) %>%
  st_combine() ->
  Wales
rm(list = c("temp", "temp2"))

## Download the visual and sensory aspect of the LANDMAP data  
VS <- read_sf("http://lle.gov.wales/catalogue/item/LandmapVisualSensory.json") %>%
  st_make_valid() %>%
  mutate(ScenicQuality = factor(VS_46, levels = c("Low", "Moderate", "High", "Outstanding")),
             Integrity = factor(VS_47, levels = c("Low", "Moderate", "High", "Outstanding")),
             Character = factor(VS_48, levels = c("Low", "Moderate", "High", "Outstanding")),
                Rarity = factor(VS_49, levels = c("Low", "Moderate", "High", "Outstanding"))) 

## Download Scenic-Or-Not data
sc1 <- read_tsv("http://scenicornot.datasciencelab.co.uk/votes.tsv",
               col_types = cols("ID" = col_number(),
                                "Lat" = col_double(),
                                "Lon" = col_double(),
                                "Average" = col_double(),
                                "Variance" = col_double(),
                                "Votes" = col_character(),
                                "Geograph URI" = col_character())) %>%
  rowwise() %>%
  mutate(Mean = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% mean(),
         Median = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% median(),
         Variance = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% var(),
         Entropy = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% entropy(),
         Number = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% length(),
           `10` = Votes %>% strsplit(",") %>% unlist() %>% str_count("10") %>% sum(),
            `9` = Votes %>% strsplit(",") %>% unlist() %>% str_count("9") %>% sum(),
            `8` = Votes %>% strsplit(",") %>% unlist() %>% str_count("8") %>% sum(),
            `7` = Votes %>% strsplit(",") %>% unlist() %>% str_count("7") %>% sum(),
            `6` = Votes %>% strsplit(",") %>% unlist() %>% str_count("6") %>% sum(),
            `5` = Votes %>% strsplit(",") %>% unlist() %>% str_count("5") %>% sum(),
            `4` = Votes %>% strsplit(",") %>% unlist() %>% str_count("4") %>% sum(),
            `3` = Votes %>% strsplit(",") %>% unlist() %>% str_count("3") %>% sum(),
            `2` = Votes %>% strsplit(",") %>% unlist() %>% str_count("2") %>% sum(),
            `1` = Votes %>% strsplit(",") %>% unlist() %>% str_count("1") %>% sum())%>%
  mutate(`1` = `1`-`10`) %>%
  st_as_sf(coords = c("Lon","Lat"), crs = 4326) %>%
  st_transform(crs = 27700)
```

```{python}
import math
from scipy import stats

def ci_lower_bound(positive_ratings, total_ratings, confidence):
    """
    Following http://www.evanmiller.org/how-not-to-sort-by-average-rating.html#changes
    :return: the lower bound of the confidence interval
    """
    n =  total_ratings
    z = stats.norm.ppf(1 - (1 - confidence) / 2)
    p_hat = 1.0 * positive_ratings / n
    result = (
        (
             p_hat + z * z / (2 * n)
             - z * math.sqrt(
                 (p_hat * (1 - p_hat) + z * z / (4 * n)) / n
             )
        ) / (1 + z * z / n)
    )
    return result

```

```{r}
LANDMAP <- sc %>% 
  st_join(VS, .) %>% 
  aggregate(Votes~UID, data = ., paste0, collapse = ",") %>%
  merge(VS, ., by = "UID", all = T) %>%
  as_tibble() %>% 
  st_as_sf() %>%
  rowwise() %>%
  mutate(Scenicness.mean_all = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% mean(),
         Scenicness.median_all = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% median(),
         Scenicness.entropy_all = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% entropy(),
         Scenicness.number_all = Votes %>% strsplit(",") %>% unlist() %>% as.numeric() %>% na.omit() %>% length()) %>%
  ungroup()

LANDMAP <- sc %>%
  st_join(LANDMAP, .) %>%
  aggregate(Median~UID, data = ., paste0, collapse = ",") %>%
  merge(LANDMAP, ., by = "UID", all = T) %>%
  as_tibble() %>%
  st_as_sf() %>%
  rowwise() %>%
  mutate(Scenicness.mean_of_median = Median %>% strsplit(",") %>% unlist() %>% as.numeric() %>% mean(),
         Scenicness.median_of_median = Median %>% strsplit(",") %>% unlist() %>% as.numeric() %>% median()) %>%
  ungroup()

LANDMAP <- sc %>%
  st_join(LANDMAP, .) %>%
  aggregate(Mean~UID, data = ., paste0, collapse = ",") %>%
  merge(LANDMAP, ., by = "UID", all = T) %>%
  as_tibble() %>%
  st_as_sf() %>%
  rowwise() %>%
  mutate(Scenicness.mean_of_mean = Mean %>% strsplit(",") %>% unlist() %>% as.numeric() %>% mean(),
         Scenicness.median_of_mean = Mean %>% strsplit(",") %>% unlist() %>% as.numeric() %>% median(),
         Consultant = ifelse(str_detect(VS_1a, "Bronwen Thomas"), "B", "A")) %>%
  dplyr::select(UID, CLS_1, CLS_2, CLS_3, CLS_4, ScenicQuality, Consultant, 
                Votes, Scenicness.mean_all, Scenicness.median_all, Scenicness.entropy_all, Scenicness.number_all,
                Scenicness.mean_of_median, Scenicness.median_of_median,
                Scenicness.mean_of_mean, Scenicness.median_of_mean) %>%
  ungroup()

plot(LANDMAP["ScenicQuality"], border = 'NA', key.pos = 1)  
```

Add wildness measures

```{r}
library(exactextractr)

Wildness <- raster('Z:\\R\\R projects\\LCA-GB\\James\\mce_eq256')

LANDMAP <- st_transform(LANDMAP, crs = st_crs(Wildness)) %>%
  exact_extract(Wildness, ., fun = c('mean', 'median', 'coefficient_of_variation', 'variance', 'quantile'), quantiles = c(0.25, 0.75)) %>%
  mutate(Wildness.iqr = q75 - q25) %>%
  dplyr::select(Wildness.mean = mean, Wildness.median = median, Wildness.iqr, Wildness.cv = coefficient_of_variation, Wildness.variance = variance) %>%
  bind_cols(LANDMAP[, 1:13], .) %>%
  st_transform(crs = 27700)

landmap <- st_cast(LANDMAP, "POLYGON") %>%
  dplyr::mutate(x = st_centroid(landmap$geometry) %>% st_coordinates() %>% .[, 1],
                y = st_centroid(landmap$geometry) %>% st_coordinates() %>% .[, 2])
summary(landmap)
```

```{r}
Sys.setenv("RETICULATE_PYTHON" = ".\\.venv\\Scripts\\python.exe")
# Sys.getenv()
Sys.which("Python")

# py_discover_config()

library(reticulate)
# -- creates an interactive Python console, REPL (Read, Eval, Print Loop), within R.
repl_python() 

# reticulate::py_config() # why numpy_version couldn't be found?
# use_python("Z:\\R\\R projects\\LCA-GB\\my_env_Python_3.9.13\\Scripts\\python.exe")
# 
# # use_virtualenv("Z:\\R\\R projects\\LCA-GB\\my_env_Python_3.9.13", required = TRUE)
# 
# install.packages("usethis")
# usethis::edit_r_environ()
# usethis::edit_r_profile()
```

```{python}
import geopandas as gpd
import pandas as pd
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import make_column_transformer

landmap = r.landmap
landmap = landmap.dropna(subset=['ScenicQuality'])

print(landmap)
landmap.describe()
print(landmap['ScenicQuality'].value_counts())

landmap.isnull().sum()

landmap_A = landmap.query("Consultant in ['A']")
landmap_B = landmap.query("Consultant in ['B']")

print(landmap.shape)
print(landmap_A.shape)
print(landmap_B.shape)

# def split_consultant(landmap):
#   X = landmap.loc[:, ['Scenicness.mean', 'Scenicness.entropy', 'Wildness.median', 'Wildness.iqr']]
#   y = landmap.loc[:, ['ScenicQuality']]
# 
#   # Instantiate the label encoder
#   transformer = make_column_transformer(
#     (OrdinalEncoder(categories=[['Low', 'Moderate', 'High', 'Outstanding']]), ['ScenicQuality'])
#   )
# 
#   y = transformer.fit_transform(y)
#   
#   y = pd.DataFrame(y, columns = ['ScenicQuality'])
#   return X, y
# 
# def EDA_plot(y, X):
#   import pandas as pd
#   # concatenating y and X along columns
#   landmap_concat = pd.DataFrame(pd.concat([y.reset_index(drop=True), X.reset_index(drop=True)], axis=1))
#   print(landmap_concat.shape)
# 
#   import seaborn as sns
#   sns.set()
#   sns.pairplot(landmap_concat, hue='ScenicQuality', palette='vlag', height=3)

# split train/test set
# X = landmap_B.loc[:, ['Scenicness.median', 'Scenicness.entropy', 
#                       'Wildness.median', 'Wildness.variance',
#                       'x', 'y']]
                      
X = landmap_B.loc[:, ['Scenicness.mean', 'Wildness.mean']]
y = landmap_B.loc[:, ['ScenicQuality']]

# from sklearn.preprocessing import OrdinalEncoder
# OE = OrdinalEncoder()
# y[['Scale']] = OE.fit_transform(y[['ScenicQuality']])

scale_mapper = {'Low': int(0), 
                'Moderate': int(1), 
                'High': int(2), 
                'Outstanding': int(3)}
                
y[['ScenicQuality']] = y[['ScenicQuality']].replace(scale_mapper).astype('int')

# X, y = split_consultant(landmap)
print(y.shape, X.shape)
X.head()
y.head()
# EDA_plot(y, X)


# -- Create train/test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size = 0.2, 
                                                    random_state = 2046, 
                                                    stratify = y)
```
## Hyperparameter Tuning

The optimization process consists of 4 parts which are as follows:

#### 1. Define a search space

The domain space is the input values over which we want to search.
Choose hyperparameter domain to search over

-   `eta` [0,1] is analogous to learning rate in GBM conceived as the step size shrinkage.
-   `n_estimators` [X/L/C]: no. of boosting iterations
-   `gamma` [0,∞] specifies the minimum loss reduction required to make a split.
-   `max_depth` [0,∞] refers to the maximum depth of a tree, same as GBM, which is used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.
-   `min_child_weight` [0,∞] is similar to min_child_leaf in GBM but not exactly which refers to min 'sum of weights' of observations while GBM has min 'number of observations.'
-   `max_delta_step` [0,∞] is usually not needed, but it might help in logistic regression when class is extremely imbalanced.
-   `subsample` [0,1] denotes the fraction of observations to be randomly samples for each tree.
-   `colsample_bytree`
-   `lamda` denotes L2 regularization term on weights (analogous to Ridge regression).
-   `alpha` denotes L1 regularization term on weights (analogous to Lasso regression), which can be used in case of very high dimensionality so that the algorithm runs faster when implemented.
-   `tree_method`
-   `scale_pos_weight`
-   `max_leaves`

```{python}
# -- hyperopt for optimising a XGBoost model
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval
from sklearn.metrics import cohen_kappa_score
import numpy as np
import xgboost as xgb

# -- define a search space
param_space = {
  # General Parameters
           'booster': 'gbtree',
     # 'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart']),
     #     'verbosity':
     #       'nthread': 
             
  # Booster Parameters
   'num_boost_round': hp.quniform('num_boost_round', 100, 1500, 0.001), 
               'eta': hp.uniform('eta', 0.01, 1.01),
             'gamma': 0, 
         'max_depth': hp.choice('max_depth', np.arange(1, 10, 1, dtype=int)),
  'min_child_weight': hp.quniform('min_child_weight', 0, 1, 0.1),     
         'subsample': hp.quniform('subsample', 0.3, 1, 0.001),
  'colsample_bytree': 1, #hp.uniform('colsample_bytree', 0.5, 1.01),
 #       'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),
 #        'reg_alpha': hp.quniform('reg_alpha', 40, 180, 1),
 #      'tree_method': 'hist','gpu_hist',  # Use GPU accelerated algorithm
 # 'scale_pos_weight':
 #       'max_leaves':
 #     'n_estimators': hp.quniform('n_estimators', 5, 35, 1),
 #       'num_leaves': hp.quniform('num_leaves', 5, 50, 1),
 #        'predictor': 'cpu_predictor',
          
  # Learning Task Parameters
         'num_class': 4,                   # Number of possible output classes
         'objective': 'multi:softmax',     # Specify multi-class classification
       'eval_metric': 'merror',
              'seed': 2022
    }

# Sample values from these configuration spaces using the routines in hyperopt.pyll.stochastic
from hyperopt.pyll.stochastic import sample
space = {'eta': hp.uniform('eta', 0.01, 1.01)}
print(sample(space))
```
#### 2. Define an objective/loss/cost function

The objective function can be any function which returns a real value that we want to minimize. In this case, we want to minimize the validation error of a machine learning model with respect to the hyperparameters. If the real value is accuracy, then we want to maximize it. Then the function should return the negative of that metric.

```{python}
# -- define an objective function
def objective(params, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test):
    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train,
              eval_set = [(X_train, y_train), (X_test, y_test)])
    y_pred = model.predict(X_test)
    score = np.mean(cohen_kappa_score(y_test, y_pred))
    # -- return more than just the objective value
    return {'loss': score, 'status': STATUS_OK}
```

##### Implementing/Choosing the evaluation metric:

#### 3. Minimize the objective over the space (optimisation algorithm)

It is the method used to construct the surrogate objective function and choose the next values to evaluate. The optimization algorithm is based on the Sequential Model-Based Global Optimization (SMBO) methodology with the variants given by the Gaussian Processes (GP), Tree of Parzen Estimators (TPE) and Adaptive Tree of Parzen Estimators (ATPE) algorithms.

Hyperopt has the TPE option along with random search. During optimization, the TPE algorithm constructs the probability model from the past results and decides the next set of hyperparameters to evaluate in the objective function by maximizing the expected improvement.

##### Parameter Tuning: Optimizing the Cross Validated Score

We now need to put these together and call Hyperopt's optimization function, fmin, to search for the optimal parameter combination. I will define a new function that implements this optimization and returns the parameters that acheive the highest cross validated score.

The fmin function minimzes the objective function over the paramter space defined by param_space (described below). It takes a Trials object to store the results of all iterations, which can be used later to gain insights about the search process and results. For more about this (and Tree-structured Parzen Estimator, TPE, which Hyperopt uses internally for optimization) see Hyperopt documentation.

```{python}
# -- optimize it using algorithm of choice
def optimize(trials, space):
    best = fmin(fn = objective,
                space = param_space, 
                algo = tpe.suggest,    # the optimisation algorithm
                max_evals = 30,        # the number of iteration
                trials = trials)       # the trials database for fmin
    return best

# -- create a Trials database to store experiment results
trials = Trials()
best_params = optimize(trials, param_space)

# Return the best parameters
space_eval(param_space, best_params)
```
#### 4. Results

Results are score or value pairs that the algorithm uses to build the model.

## Fit xgboost model

```{python}
import xgboost as xgb
## Specify sufficient boosting iterations to reach a minimum
## num_round = 1000

# -- convert input data from numpy to XGBoost format
dtrain = xgb.DMatrix(X_train, label = y_train)
 dtest = xgb.DMatrix(X_test, label = y_test)

# -- try previous combinations of hyperparameter set
# Consultant A
# param_space = {
#            'booster': 'gbtree',
#    'num_boost_round': 209.910,
#                'eta': 0.451,
#              'gamma': 0,
#          'max_depth': 12,
#   'min_child_weight': 0.2,
#          'subsample': 0.758,
#   'colsample_bytree': 1,
#          'num_class': 4,
#          'objective': 'multi:softmax',
#        'eval_metric': 'merror',
#               'seed': 2046
#     }

# Consultant B
params = {
          'booster': 'gbtree',
        #'num_round': 1433.996,
              'eta': 0.892,
            'gamma': 0,
        'max_depth': 4,
 'min_child_weight': 0.6,
        'subsample': 0.500,
 'colsample_bytree': 1,
        'num_class': 4,
        'objective': 'multi:softmax',
      'eval_metric': 'merror',
             'seed': 1997
  }

# -- train model
import time
start = time.time()

model = xgb.train(space_eval(param_space, best_params), 
                  dtrain, 
                  num_boost_round = 500,
                  evals = [(dtrain, 'train'), (dtest, 'test')],
                  early_stopping_rounds = 250)

# model = xgb.train(params, 
#                   dtrain, 
#                   num_boost_round = 500, 
#                   evals = [(dtrain, 'train'), (dtest, 'test')],
#                   early_stopping_rounds = 250)

# model = xgb.cv(space_eval(param_space, best_params), dtrain, num_boost_round = 2000, nfold = 5, stratified = True)
# model.head()
                  
print("XGBoost model training time: %s seconds" % (str(time.time() - start)))

y_pred = model.predict(dtest)

# -- evaluate model using test data
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
 
from sklearn.metrics import cohen_kappa_score
kappa = cohen_kappa_score(y_pred, y_test)
print("Kappa: %.2f" % (kappa))

# -- save to JSON
model.save_model("Z:\\R\\R projects\\LCA-GB\\models\\model_B.json")
```

## SHAP analysis of xgboost results

```{python}
import shap
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import plot_confusion_matrix

# -- load the saved model
model = xgb.XGBRegressor()
model.load_model("Z:\\R\\R projects\\LCA-GB\\models\\model_B.json")

# -- view the confusion matrix
class_names = ['Low', 'Moderate', 'High', 'Outstanding']
disp = plot_confusion_matrix(model, X_test, y_test, 
                             display_labels = class_names, 
                             cmap = plt.cm.Blues, 
                             xticks_rotation = 'vertical')

# -- view the feature importance
importances = model.feature_importances_
indices = np.argsort(importances)
features = X.columns

fig, ax = plt.subplots()

plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='g', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

fig.set_size_inches(18.5, 10.5)
fig.savefig("Z:\\R\\R projects\\LCA-GB\\models\\pyplot.png")
plt.clf()

# -- play shap
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test, approximate = True)
shap.summary_plot(shap_values[0], X_test.values, feature_names = X_test.columns)
fig.set_size_inches(18.5, 20.5)
fig.savefig("Z:\\R\\R projects\\LCA-GB\\models\\shap.png")

# -- first trial
shap.initjs()
shap_values = shap.TreeExplainer(model).shap_values(X_test)
shap.summary_plot(shap_values, X_test)

# -- second trial
scaler = StandardScaler()
scaled_data = scaler.fit_transform(X_test.iloc[:,:-1])
shap_values = explainer.shap_values(scaled_data.iloc[0,:-1].values.reshape(1,-1))
shap.force_plot(explainer.expected_value[0], shap_values[0], link='logit')  # repeat changing 0 for i in range(0, 5)

# -- third trial
shap.initjs()
class_names = ['Low', 'Moderate', ' High', 'Outstanding']
shap.summary_plot(shap_values, X_test, plot_type="bar", class_names= class_names, feature_names = ['Scenicness', 'Wildness'])

# -- plot the global importance of each feature
shap.plots.bar(explainer)

# visualize the first prediction's explanation
shap.force_plot(explainer.expected_value[0], shap_values[0])

# fig, ax = plt.subplots(figsize=(20, 20),dpi=200)
# shap.summary_plot(shap_values)
# # or 
# shap.plots.beeswarm(shap_values)
shap.summary_plot(shap_values[2], X_test)

# fig, ax = plt.subplots(figsize=(20, 20),dpi=200)
shap.summary_plot(shap_values, X_test, plot_type="bar")

shap_interaction_values = shap.TreeExplainer(model).shap_interaction_values(X_train)
    
X_names = ['Scenicness.median','x_coord','y_coord']

shap.summary_plot(shap_interaction_values, X_train, max_display=15, 
                  feature_names = X_names,
                  plot_type = 'dot')
                  # plot_type="compact_dot")
                  
import shap  # package used to calculate Shap values

# Create object that can calculate shap values
explainer = shap.TreeExplainer(my_model)

# calculate shap values. This is what we will plot.
# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values = explainer.shap_values(val_X)

# Make plot. Index of [1] is explained in text below.
shap.summary_plot(shap_values[1], val_X)                  
```
